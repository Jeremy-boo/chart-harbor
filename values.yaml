global:
  registry:
    address: harbor.alauda.cn
  images:
    portal:
      repository: devops/goharbor-harbor-portal
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    core:
      repository: devops/goharbor-harbor-core
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    migrator:
      repository: devops/harbor-migrator
      tag: v3.9-2-ge888f0a
      support_arm: true
      code: gitlab-ce.alauda.cn/devops/harbor-migrator
    jobservice:
      repository: devops/goharbor-harbor-jobservice
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    registryRegistry:
      repository: devops/goharbor-registry-photon
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    registryController:
      repository: devops/goharbor-harbor-registryctl
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    chartmuseum:
      repository: devops/goharbor-chartmuseum-photon
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    trivy:
      repository: devops/goharbor-trivy-adapter-photon
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    trivyOfflineDB:
      repository: devops/goharbor-trivy-offline-db
      tag: alauda-v2.2.3-127
      support_arm: true
      thirdparty: true
    notaryServer:
      repository: devops/goharbor-notary-server-photon
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    notarySigner:
      repository: devops/goharbor-notary-signer-photon
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    nginx:
      repository: devops/goharbor-nginx-photon
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    db:
      repository: devops/goharbor-harbor-db
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    exporter:
      repository: devops/goharbor-harbor-exporter
      tag: alauda-v2.2.3-128
      support_arm: true
      thirdparty: true
    redis:
      repository: devops/redis
      tag: alauda-5.0.13-5
      support_arm: true
      thirdparty: true
    devopsCLI:
      repository: devops/devops-cli
      tag: v3.9-91-g8200ae90e-dirty
      support_arm: true
      code: gitlab-ce.alauda.cn/devops/devops-apiserver
    initContainer:
      repository: ops/busybox
      tag: stable
      support_arm: true
      thirdparty: true
  labelBaseDomain: alauda.io
  # alauda global-secret
  globalTLSSecretName: ""
  statefulset:
    db:
      podManagementPolicy: Parallel
    redis:
      podManagementPolicy: Parallel
    trivy:
      podManagementPolicy: Parallel
resources: &resources
  limits:
    cpu: 2
    memory: 4Gi
  requests:
    cpu: 256m
    memory: 256Mi
Auth:
  mode: ""
  oidcClientID: ""
  oidcClientSecret: ""
  oidcEndpoint: ""
  oidcName: ""
  oidcScope: ""
  oidcVerifyCert: false
  harborURL: ""
oidc:
  enable: false
  clientID: ""
  clientSecret: ""
  issuer: ""
  scope: ""
  serverURL: ""
  verifyCert: false
  harbor:
    mode: ""
    oidcName: ""
AlaudaACP:
  Enabled: false
  Name: harbor-registry
# High available
highAvailability:
  enable: false
expose:
  # Set the way how to expose the service. Set the type as "ingress",
  # "clusterIP", "nodePort" or "loadBalancer" and fill the information
  # in the corresponding section
  type: nodePort
  tls:
    # Enable the tls or not. Note: if the type is "ingress" and the tls
    # is disabled, the port must be included in the command when pull/push
    # images. Refer to https://github.com/goharbor/harbor/issues/5291
    # for the detail.
    enabled: false
    # The source of the tls certificate. Set it as "auto", "secret"
    # or "none" and fill the information in the corresponding section
    # 1) auto: generate the tls certificate automatically
    # 2) secret: read the tls certificate from the specified secret.
    # The tls certificate can be generated manually or by cert manager
    # 3) none: configure no tls certificate for the ingress. If the default
    # tls certificate is configured in the ingress controller, choose this option
    certSource: auto
    # Fill the name of secret if you want to use your own TLS certificate.
    # The secret contains keys named:
    # "tls.crt" - the certificate (required)
    # "tls.key" - the private key (required)
    # "ca.crt" - the certificate of CA (optional), this enables the download
    # link on portal to download the certificate of CA
    # These files will be generated automatically if the "secretName" is not set
    secretName: ""
    # By default, the Notary service will use the same cert and key as
    # described above. Fill the name of secret if you want to use a
    # separated one. Only needed when the type is "ingress".
    notarySecretName: ""
    # The common name used to generate the certificate, it's necessary
    # when the type isn't "ingress" and "secretName" is null
    commonName: ""
  ingress:
    hosts:
      core: core.harbor.domain
      notary: notary.harbor.domain
    # set to the type of ingress controller if it has specific requirements.
    # leave as `default` for most ingress controllers.
    # set to `gce` if using the GCE ingress controller
    # set to `ncp` if using the NCP (NSX-T Container Plugin) ingress controller
    controller: default
    annotations:
      ingress.kubernetes.io/ssl-redirect: "true"
      ingress.kubernetes.io/proxy-body-size: "0"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/proxy-body-size: "0"
  clusterIP:
    name: harbor
    ports:
      httpPort: 80
      httpsPort: 443
      notaryPort: 4443
  nodePort:
    name: harbor
    ports:
      http:
        port: 80
        nodePort: 31104
      https:
        port: 443
        nodePort: 31105
      notary:
        port: 4443
        nodePort: 31106
  loadBalancer:
    name: harbor
    IP: ""
    ports:
      httpPort: 80
      httpsPort: 443
      notaryPort: 4443
    annotations: {}
    sourceRanges: []
# The external URL for Harbor core service. It is used to
# 1) populate the docker/helm commands showed on portal
# 2) populate the token service URL returned to docker/notary client
#
# Format: protocol://domain[:port]. Usually:
# 1) if "expose.type" is "ingress", the "domain" should be
# the value of "expose.ingress.hosts.core"
# 2) if "expose.type" is "clusterIP", the "domain" should be
# the value of "expose.clusterIP.name"
# 3) if "expose.type" is "nodePort", the "domain" should be
# the IP address of k8s node
#
# If Harbor is deployed behind the proxy, set it as the URL of proxy
externalURL: https://core.harbor.domain
# The internal TLS used for harbor components secure communicating. In order to enable https
# in each components tls cert files need to provided in advance.
internalTLS:
  # If internal TLS enabled
  enabled: false
  # There are three ways to provide tls
  # 1) "auto" will generate cert automatically
  # 2) "manual" need provide cert file manually in following value
  # 3) "secret" internal certificates from secret
  certSource: "auto"
  # The content of trust ca, only available when `certSource` is "manual"
  trustCa: ""
  # core related cert configuration
  core:
    # secret name for core's tls certs
    secretName: ""
    # Content of core's TLS cert file, only available when `certSource` is "manual"
    crt: ""
    # Content of core's TLS key file, only available when `certSource` is "manual"
    key: ""
  # jobservice related cert configuration
  jobservice:
    # secret name for jobservice's tls certs
    secretName: ""
    # Content of jobservice's TLS key file, only available when `certSource` is "manual"
    crt: ""
    # Content of jobservice's TLS key file, only available when `certSource` is "manual"
    key: ""
  # registry related cert configuration
  registry:
    # secret name for registry's tls certs
    secretName: ""
    # Content of registry's TLS key file, only available when `certSource` is "manual"
    crt: ""
    # Content of registry's TLS key file, only available when `certSource` is "manual"
    key: ""
  # portal related cert configuration
  portal:
    # secret name for portal's tls certs
    secretName: ""
    # Content of portal's TLS key file, only available when `certSource` is "manual"
    crt: ""
    # Content of portal's TLS key file, only available when `certSource` is "manual"
    key: ""
  # chartmuseum related cert configuration
  chartmuseum:
    # secret name for chartmuseum's tls certs
    secretName: ""
    # Content of chartmuseum's TLS key file, only available when `certSource` is "manual"
    crt: ""
    # Content of chartmuseum's TLS key file, only available when `certSource` is "manual"
    key: ""
  # trivy related cert configuration
  trivy:
    # secret name for trivy's tls certs
    secretName: ""
    # Content of trivy's TLS key file, only available when `certSource` is "manual"
    crt: ""
    # Content of trivy's TLS key file, only available when `certSource` is "manual"
    key: ""
# The persistence is enabled by default and a default StorageClass
# is needed in the k8s cluster to provision volumes dynamicly.
# Specify another StorageClass in the "storageClass" or set "existingClaim"
# if you have already existing persistent volumes to use
#
# For storing images and charts, you can also use "azure", "gcs", "s3",
# "swift" or "oss". Set it in the "imageChartStorage" section
persistence:
  enabled: false
  resourcePolicy: keep
  hostPath:
    registry:
      host:
        nodeName: null
        path: /tmp/harbor/registry
    chartmuseum:
      host:
        nodeName: null
        path: /tmp/harbor/chartmuseum
    jobservice:
      host:
        nodeName: null
        path: /tmp/harbor/jobservice
    database:
      host:
        nodeName: null
        path: /tmp/harbor/database
    redis:
      host:
        nodeName: null
        path: /tmp/harbor/redis
    trivy:
      host:
        nodeName: null
        path: /tmp/harbor/trivy
  persistentVolumeClaim:
    registry:
      # Use the existing PVC which must be created manually before bound,
      # and specify the "subPath" if the PVC is shared with other components
      existingClaim: ""
      # Specify the "storageClass" used to provision the volume. Or the default
      # StorageClass will be used(the default).
      # Set it to "-" to disable dynamic provisioning
      storageClass: ""
      subPath: ""
      accessMode: ReadWriteOnce
      size: 5Gi
    chartmuseum:
      existingClaim: ""
      storageClass: ""
      subPath: ""
      accessMode: ReadWriteOnce
      size: 5Gi
    jobservice:
      existingClaim: ""
      storageClass: ""
      subPath: ""
      accessMode: ReadWriteOnce
      size: 1Gi
    database:
      existingClaim: ""
      storageClass: ""
      subPath: ""
      accessMode: ReadWriteOnce
      size: 1Gi
    redis:
      existingClaim: ""
      storageClass: ""
      subPath: ""
      accessMode: ReadWriteOnce
      size: 1Gi
    trivy:
      existingClaim: ""
      storageClass: ""
      subPath: ""
      accessMode: ReadWriteOnce
      size: 5Gi
  # Define which storage backend is used for registry and chartmuseum to store
  # images and charts. Refer to
  # https://github.com/docker/distribution/blob/master/docs/configuration.md#storage
  # for the detail.
  imageChartStorage:
    # Specify whether to disable `redirect` for images and chart storage, for
    # backends which not supported it (such as using minio for `s3` storage type), please disable
    # it. To disable redirects, simply set `disableredirect` to `true` instead.
    # Refer to
    # https://github.com/docker/distribution/blob/master/docs/configuration.md#redirect
    # for the detail.
    disableredirect: false
    type: filesystem
    filesystem:
      rootdirectory: /var/lib/registry
    azure:
      accountname: accountname
      accountkey: base64encodedaccountkey
      container: containername
    gcs:
      bucket: bucketname
      encodedkey: base64-encoded-json-key-file
    s3:
      region: us-west-1
      bucket: bucketname
      # accesskey: awsaccesskey
      # secretkey: awssecretkey
      # regionendpoint: http://myobjects.local
      # encrypt: false
      # keyid: mykeyid
      # secure: true
      # v4auth: true
      # chunksize: "5242880"
      # rootdirectory: /s3/object/name/prefix
      # storageclass: STANDARD
    swift:
      authurl: https://storage.myprovider.com/v3/auth
      username: username
      password: password
      container: containername
      # region: fr
      # tenant: tenantname
      # tenantid: tenantid
      # domain: domainname
      # domainid: domainid
      # trustid: trustid
      # insecureskipverify: false
      # chunksize: 5M
      # prefix:
      # secretkey: secretkey
      # accesskey: accesskey
      # authversion: 3
      # endpointtype: public
      # tempurlcontainerkey: false
      # tempurlmethods:
    oss:
      accesskeyid: accesskeyid
      accesskeysecret: accesskeysecret
      region: regionname
      bucket: bucketname
      # endpoint: endpoint
      # internal: false
      # encrypt: false
      # secure: true
      # chunksize: 10M
      # rootdirectory: rootdirectory
imagePullPolicy: IfNotPresent
imagePullSecrets: null
#  - name: docker-registry-secret
#  - name: internal-registry-secret

# The update strategy for deployments with persistent volumes(jobservice, registry
# and chartmuseum): "RollingUpdate" or "Recreate"
# Set it as "Recreate" when "RWM" for volumes isn't supported
updateStrategy:
  type: RollingUpdate
logLevel: info
harborAdminPassword: Harbor12345
harborAdminPasswordRef: ""
secretKey: not-a-secure-key
proxy:
  httpProxy: null
  httpsProxy: null
  noProxy: 127.0.0.1,localhost,.local,.internal
  components:
    - core
    - jobservice
    - trivy
nginx:
  replicas: 1
  serviceAccountName: ""
  resources:
    !!merge <<: *resources
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
portal:
  replicas: 1
  serviceAccountName: ""
  resources:
    !!merge <<: *resources
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
core:
  replicas: 1
  startupProbe:
    enabled: true
    initialDelaySeconds: 10
  livenessProbe:
    initialDelaySeconds: 300
  resources:
    !!merge <<: *resources
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
  # Secret is used when core server communicates with other components.
  # If a secret key is not specified, Helm will generate one.
  # Must be a string of 16 chars.
  # Give default values to avoid frequent changes to `/core/core-secret.yaml`
  secret: "0123456789ABCDEF"
  # Fill the name of a kubernetes secret if you want to use your own
  # TLS certificate and private key for token encryption/decryption.
  # The secret must contain keys named:
  # "tls.crt" - the certificate
  # "tls.key" - the private key
  # The default key pair will be used if it isn't set
  secretName: ""
  # The XSRF key. Will be generated automatically if it isn't specified
  # must be 32 length. so use md5('harbor') as default
  xsrfKey: "A3F7B4175EB3C1DA2F477685808422A5"
jobservice:
  replicas: 1
  serviceAccountName: ""
  maxJobWorkers: 10
  # The logger for jobs: "file", "database" or "stdout"
  jobLoggers:
    - file
  resources:
    !!merge <<: *resources
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
  secret: ""
registry:
  serviceAccountName: ""
  registry:
    image: null
    resources:
      !!merge <<: *resources
  controller:
    image: null
    resources:
      !!merge <<: *resources
  replicas: 1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
  secret: ""
  relativeurls: false
  credentials:
    username: "harbor_registry_user"
    password: "harbor_registry_password"
    # If you update the username or password of registry, make sure use cli tool htpasswd to generate the bcrypt hash
    # e.g. "htpasswd -nbBC10 $username $password"
    htpasswd: "harbor_registry_user:$2y$10$9L4Tc0DJbFFMB6RdSCunrOpTHdwhid4ktBJmLD00bYgqkkGOvll3m"
  middleware:
    enabled: false
    type: cloudFront
    cloudFront:
      baseurl: example.cloudfront.net
      keypairid: KEYPAIRID
      duration: 3000s
      ipfilteredby: none
      privateKeySecret: my-secret
chartmuseum:
  serviceAccountName: ""
  enabled: true
  # Harbor defaults ChartMuseum to returning relative urls, if you want using absolute url you should enable it by change the following value to 'true'
  absoluteUrl: false
  replicas: 1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
trivy:
  # enabled the flag to enable Trivy scanner
  enabled: true
  image:
  # set the service account to be used, default if left empty
  serviceAccountName: ""
  # replicas the number of Pod replicas
  replicas: 1
  # debugMode the flag to enable Trivy debug mode with more verbose scanning log
  debugMode: false
  # vulnType a comma-separated list of vulnerability types. Possible values are `os` and `library`.
  vulnType: "os,library"
  # severity a comma-separated list of severities to be checked
  severity: "UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL"
  # ignoreUnfixed the flag to display only fixed vulnerabilities
  ignoreUnfixed: false
  # insecure the flag to skip verifying registry certificate
  insecure: false
  # gitHubToken the GitHub access token to download Trivy DB
  #
  # Trivy DB contains vulnerability information from NVD, Red Hat, and many other upstream vulnerability databases.
  # It is downloaded by Trivy from the GitHub release page https://github.com/aquasecurity/trivy-db/releases and cached
  # in the local file system (`/home/scanner/.cache/trivy/db/trivy.db`). In addition, the database contains the update
  # timestamp so Trivy can detect whether it should download a newer version from the Internet or use the cached one.
  # Currently, the database is updated every 12 hours and published as a new release to GitHub.
  #
  # Anonymous downloads from GitHub are subject to the limit of 60 requests per hour. Normally such rate limit is enough
  # for production operations. If, for any reason, it's not enough, you could increase the rate limit to 5000
  # requests per hour by specifying the GitHub access token. For more details on GitHub rate limiting please consult
  # https://developer.github.com/v3/#rate-limiting
  #
  # You can create a GitHub token by following the instructions in
  # https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line
  gitHubToken: ""
  # skipUpdate the flag to disable Trivy DB downloads from GitHub
  #
  # You might want to set the value of this flag to `true` in test or CI/CD environments to avoid GitHub rate limiting issues.
  # If the value is set to `true` you have to manually download the `trivy.db` file and mount it in the
  # `/home/scanner/.cache/trivy/db/trivy.db` path.
  skipUpdate: false
  # running trivy with offline mod, if this flag is true
  offline: false
  # Deprecated, this property will be deleted soon
  # Required when online is false, using as CronJob schedule, default value: 0 16 * * * (UTC)
  dbUpdateSchedule: "0 16 * * *"
  resources:
    !!merge <<: *resources
  ## Additional deployment annotations
  podAnnotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
notary:
  enabled: true
  server:
    replicas: 1
    serviceAccountName: ""
    resources:
      !!merge <<: *resources
  signer:
    replicas: 1
    serviceAccountName: ""
    resources:
      !!merge <<: *resources
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
  # Fill the name of a kubernetes secret if you want to use your own
  # TLS certificate authority, certificate and private key for notary
  # communications.
  # The secret must contain keys named ca.crt, tls.crt and tls.key that
  # contain the CA, certificate and private key.
  # They will be generated if not set.
  secretName: ""
database:
  type: internal
  internal:
    serviceAccountName: ""
    password: changeit
    resources:
      !!merge <<: *resources
    nodeSelector: {}
    tolerations: []
    affinity: {}
  external:
    host: 192.168.0.1
    port: "5432"
    username: postgres
    password: password
    coreDatabase: registry
    notaryServerDatabase: notary_server
    notarySignerDatabase: notary_signer
    sslmode: disable
    nodeSelector: {}
    tolerations: []
  maxIdleConns: 50
  # The maximum number of open connections to the database.
  # If it <= 0, then there is no limit on the number of open connections.
  # Note: the default number of connections is 100 for postgre.
  maxOpenConns: 3000
  podAnnotations: {}
redis:
  type: internal
  internal:
    serviceAccountName: ""
    usePassword: false
    resources:
      !!merge <<: *resources
    password: ""
    nodeSelector: {}
    tolerations: []
    affinity: {}
  external:
    # support redis, redis+sentinel
    # addr for redis: <host_redis>:<port_redis>
    # addr for redis+sentinel: <host_sentinel1>:<port_sentinel1>,<host_sentinel2>:<port_sentinel2>,<host_sentinel3>:<port_sentinel3>
    addr: "192.168.0.2:6379"
    # The name of the set of Redis instances to monitor, it must be set to support redis+sentinel
    sentinelMasterSet: ""
    # The "coreDatabaseIndex" must be "0" as the library Harbor
    # used doesn't support configuring it
    coreDatabaseIndex: "0"
    jobserviceDatabaseIndex: "1"
    registryDatabaseIndex: "2"
    chartmuseumDatabaseIndex: "3"
    trivyAdapterIndex: "5"
    password: ""
    nodeSelector: {}
    tolerations: []
  podAnnotations: {}
exporter:
  replicas: 1
  # resources:
  #  requests:
  #    memory: 256Mi
  #    cpu: 100m
  podAnnotations: {}
  serviceAccountName: ""
  nodeSelector: {}
  tolerations: []
  affinity: {}
  cacheDuration: 30
  cacheCleanInterval: 14400
metrics:
  enabled: false
  core:
    path: /metrics
    port: 8001
  registry:
    path: /metrics
    port: 8001
  exporter:
    path: /metrics
    port: 8001
